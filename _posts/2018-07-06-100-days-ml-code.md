---
title:  "100 Days Of Machine Learning Code"
date:   2018-07-06 23:22:00 +0200
last_modified_at: 2018-07-08T23:36:00+02:00 
categories: General 
---
Here I pledge to dedicate 1 hour to coding or learning Machine Learning for the next 100 days!
## Day 0: July 06, 2018 

**Today's Progress**: 

Learned about word embedding in Sequence Models course on [Coursera](www.coursera.org), such as Word2Vec, negative sampling, GloVe word vectors and sentiment classification.

**Thoughts:** 

It is interesting to know about how the word embedding algorithms are simplified over time while still being able to achieve very good results. Being simpler in this case gets even better performance.

**Link to work:** None 

## Day 1: July 07, 2018 

**Today's Progress**: 
* Learned basics and intuitions about Singular Vector Decomposition(SVD). Here are two very good videos explaining it:
    * [How to do Singular Value Decomposition](https://www.youtube.com/watch?v=EfZsEFhHcNM): step for step explanation.
    * [Dimensionality Reduction: Singular Value Decomposition](https://www.youtube.com/watch?v=P5mlg91as1c): intuitive practical example.
* Continued with `Sequence Models` course on Coursera.
    * Learned [Debiasing word embeddings](https://www.coursera.org/lecture/nlp-sequence-models/debiasing-word-embeddings-zHASj). Concepts about how to debias the biased word embeddings. For example, "doctor" should not have a closer relation to "Man" than "Woman". And "mother" and "father" should be equally similar at the "gender" axis.
    * Programming assignments
        * Debiasing word embeddings.
        * Emojify (word embedding + LSTM) - outputs a suitable emoji given a sentence. 

**Thoughts:** 
* `SVD` is so simple yet powerful. I should try to use it to solve some real problems.
* I am still not familiar with `Keras` syntax. 
* It is quite impressive to see how well the "Emojify" works even with a relatively small data set.

**Link to work:** 

Due to the honor code on Coursera, I am not allow to share my code in the course publicly. 

## Day 1: July 08, 2018 

**Today's Progress**: 
* Learned how t-SNE works by watching [StatQuest: t-SNE, Clearly Explained](https://www.youtube.com/watch?v=NEaUSP4YerM).
* Continued with `Sequence Models` course on Coursera.
    * [Basic Models](https://www.youtube.com/watch?v=JNyDLARW9x4)
    * [Picking the most likely sentence](https://www.youtube.com/watch?v=xfplLFXsdjc)
    * [Beam Search](https://www.youtube.com/watch?v=RLWuzLLSIgw)
    * [Refinements to Beam Search](https://www.youtube.com/watch?v=gb__z7LlN_4)
    * [Error analysis in Beam Search](https://www.youtube.com/watch?v=gGw_YS7qCdg)
    * [Bleu Score](https://www.youtube.com/watch?v=DejHQYAGb7Q)
    * [Attention Model Intuition](https://www.youtube.com/watch?v=SysgYptB198)
    * [Attention Model](https://www.youtube.com/watch?v=quoGRI-1l0A)

**Thoughts:** 
* Beam Search seems like an engineering approach to cope with data set where other exact algorithms like BFS and DFS will be too time consuming. Are there really no better way than Beam Search?
* From the error analysis in beam search, I learned the concept of how to prioritize error sources, in this case in beam search or in RNN itself. Similarly, when training a Neural Network, it also has to be determined whether it is overfitting or underfitting to locate the most important error sources.

**Link to work:** None
